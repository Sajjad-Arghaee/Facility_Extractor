{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7abc2c67-286b-45ef-a396-8af958d8dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scrapy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca27adaf-2338-4d79-8180-d5cd42b42575",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/input.csv')\n",
    "validation = pd.read_csv('data/output.csv')\n",
    "\n",
    "# good article on\n",
    "# https://medium.com/@mikeyo4800/how-to-build-a-multi-label-text-classification-model-using-nlp-and-machine-learning-2e05f72aad5f\n",
    "\n",
    "facility_enum = open('data/facility_enum.txt', 'r')\n",
    "Lines = facility_enum.readlines()\n",
    "facility_enum.close()\n",
    "\n",
    "# provide new dataframe with edited tags\n",
    "filled_train = []\n",
    "# find name according to code number\n",
    "code_name_enumeration = {}\n",
    "\n",
    "for line in Lines:\n",
    "    code_split_name = list(line.split('\\t'))\n",
    "    code_split_name[1] = code_split_name[1][:-1]\n",
    "    code_name_enumeration[code_split_name[0]] = code_split_name[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83821968-f1c1-4bba-a9cc-158ae89edb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in validation.iterrows():\n",
    "    validation.at[index, 'facility_code'] = code_name_enumeration[row['facility_code']].lower().strip()\n",
    "validation.rename(columns={'facility_code': 'facility'})\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    hotel_id = row.hotel_id\n",
    "    text = row.content\n",
    "    if type(text) is not str:\n",
    "        # print(f'There is not any provided content for {hotel_id} Hotel')\n",
    "        continue\n",
    "    items = scrapy.Selector(text=text).css('.hotel-description-content::text').extract()\n",
    "    # location = items[0]\n",
    "    # facilities = items[1:]\n",
    "    facilities = items\n",
    "    merged_facilities = ''\n",
    "    for facility in facilities:\n",
    "        merged_facilities += facility + ' '\n",
    "\n",
    "    merged_facilities.lower().strip()\n",
    "    prepare_new_row = [hotel_id, merged_facilities]\n",
    "    filled_train.append(prepare_new_row)\n",
    "\n",
    "filled_train = pd.DataFrame(filled_train, columns=['hotel_id', 'facilities'])\n",
    "# print(validation)\n",
    "# print(filled_train)\n",
    "\n",
    "facility_codes = validation['facility_code'].tolist()\n",
    "unique_facility_codes = set(facility_codes)\n",
    "hotel_ids = filled_train['hotel_id'].tolist()\n",
    "unique_hotel_ids = set(hotel_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049d09c6-a974-4f1d-bb87-ed6b15d1b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_of_hotel = []\n",
    "\n",
    "for hotel_id in unique_hotel_ids:\n",
    "    hotel = validation[validation.hotel_id == hotel_id]\n",
    "    features = hotel['facility_code'].tolist()\n",
    "    features_of_hotel.append([hotel_id, features])\n",
    "\n",
    "features_df = pd.DataFrame(features_of_hotel, columns=['hotel_id', 'features'])\n",
    "# print(features_df)\n",
    "# print(filled_train)\n",
    "\n",
    "number_of_occurrence = {}\n",
    "for index, row in features_df.iterrows():\n",
    "    features_line = row['features']\n",
    "    for feature in features_line:\n",
    "        if number_of_occurrence.get(feature):\n",
    "            number_of_occurrence[feature] += 1\n",
    "        else:\n",
    "            number_of_occurrence[feature] = 1\n",
    "\n",
    "# Creating histogram\n",
    "# fig, ax = plt.subplots(figsize=(10, 7))\n",
    "# ax.hist(number_of_occurrence.values())\n",
    "# plt.show()\n",
    "# print(number_of_occurrence)\n",
    "\n",
    "number_of_occurrence_filtered = {}\n",
    "for key, value in number_of_occurrence.items():\n",
    "    if value > 10:\n",
    "        number_of_occurrence_filtered[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1202fc-9234-4de8-b3d7-8cbff64b8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(len(number_of_occurrence))\n",
    "# print(len(number_of_occurrence_filtered))\n",
    "# result => from 231 to 144 features\n",
    "selected_features = number_of_occurrence_filtered.keys()\n",
    "for index, row in features_df.iterrows():\n",
    "    new_features = []\n",
    "    for feature in row['features']:\n",
    "        if feature in selected_features:\n",
    "            new_features.append(feature)\n",
    "    features_df.at[index, 'features'] = new_features\n",
    "\n",
    "preprocessed_dataframe = pd.merge(filled_train['facilities'], features_df['features'], left_index=True,\n",
    "                                  right_index=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_dataframe['facilities'],\n",
    "                                                    preprocessed_dataframe['features'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c0b713-ac32-461b-ac7b-a59432aa1634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facilities</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This hotel is located in Păulestii Noi. The fr...</td>\n",
       "      <td>[category (official), category (recommended), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This hotel is located in Guarene. The hotel ha...</td>\n",
       "      <td>[category (official), number of floors (main b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This hotel is located right in the heart of Mi...</td>\n",
       "      <td>[category (official), total number of rooms, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This hotel warmly welcomes guests in Ermington...</td>\n",
       "      <td>[category (official), total number of rooms, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This accommodation is located in Budapest. Thi...</td>\n",
       "      <td>[category (official), category (recommended), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>This hotel warmly welcomes guests in Rye. For ...</td>\n",
       "      <td>[category (official), category (recommended), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>This complex warmly welcomes guests in Jakarta...</td>\n",
       "      <td>[category (official), category (recommended), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>This hotel is located in Abu Dhabi, right on t...</td>\n",
       "      <td>[category (official), year of construction, nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Set exclusively on a private peninsula and sta...</td>\n",
       "      <td>[category (official), year of construction, nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>This hotel welcomes guests in Luís Correia. Th...</td>\n",
       "      <td>[category (official), total number of rooms, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            facilities   \n",
       "0    This hotel is located in Păulestii Noi. The fr...  \\\n",
       "1    This hotel is located in Guarene. The hotel ha...   \n",
       "2    This hotel is located right in the heart of Mi...   \n",
       "3    This hotel warmly welcomes guests in Ermington...   \n",
       "4    This accommodation is located in Budapest. Thi...   \n",
       "..                                                 ...   \n",
       "973  This hotel warmly welcomes guests in Rye. For ...   \n",
       "974  This complex warmly welcomes guests in Jakarta...   \n",
       "975  This hotel is located in Abu Dhabi, right on t...   \n",
       "976  Set exclusively on a private peninsula and sta...   \n",
       "977  This hotel welcomes guests in Luís Correia. Th...   \n",
       "\n",
       "                                              features  \n",
       "0    [category (official), category (recommended), ...  \n",
       "1    [category (official), number of floors (main b...  \n",
       "2    [category (official), total number of rooms, v...  \n",
       "3    [category (official), total number of rooms, a...  \n",
       "4    [category (official), category (recommended), ...  \n",
       "..                                                 ...  \n",
       "973  [category (official), category (recommended), ...  \n",
       "974  [category (official), category (recommended), ...  \n",
       "975  [category (official), year of construction, nu...  \n",
       "976  [category (official), year of construction, nu...  \n",
       "977  [category (official), total number of rooms, a...  \n",
       "\n",
       "[978 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3331f477-94fb-4888-9eb0-dd4e7dedb85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 100)          230400    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 96, 128)           64128     \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 302,849\n",
      "Trainable params: 302,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Step 6: Train the Model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Step 7: Evaluate the Model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, np\u001b[38;5;241m.\u001b[39marray(y_test))\n",
      "File \u001b[1;32mE:\\uni files\\semester6\\Projects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\uni files\\semester6\\Projects\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess the Text Data\n",
    "\n",
    "# (Perform necessary preprocessing steps on the 'text' column of the data)\n",
    "\n",
    "# Step 3: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_dataframe['facilities'])\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_dataframe['facilities'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = 100  # Maximum sequence length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Step 4: Split the Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, preprocessed_dataframe['features'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Build the CNN Model\n",
    "embedding_dim = 100  # Dimensionality of the word embeddings\n",
    "filters = 128  # Number of filters in the convolutional layer\n",
    "kernel_size = 5  # Size of the filters\n",
    "hidden_dims = 64  # Number of neurons in the dense layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Step 6: Train the Model\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "model.fit(np.array(X_train), np.array(y_train), validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# Step 7: Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, np.array(y_test))\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 8: Predict Tags\n",
    "new_text = [\"New text example\"]  # New, unseen text data\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_len, padding='post')\n",
    "predicted_tags = model.predict_classes(new_padded_sequences)\n",
    "print(f\"Predicted tags: {predicted_tags}\")\n",
    "\n",
    "# Step 9: Iterate and Improve\n",
    "# (Perform further iterations and improvements as necessary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
